{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0d19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19a5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026b1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "import spacy\n",
    "import openai\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from mega.data.load_datasets import load_xnli_dataset\n",
    "from mega.data.data_utils import choose_few_shot_examples\n",
    "from mega.prompting.instructions import INSTRUCTIONS\n",
    "from mega.prompting.prompting_utils import load_prompt_template\n",
    "from mega.utils.env_utils import load_env\n",
    "from mega.models.completion_models import get_model_pred, gpt3x_completion\n",
    "from mega.prompting.prompting_utils import construct_prompt, construct_qa_prompt\n",
    "from tqdm import tqdm\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ef56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that {env_name}.env file is present in the envs/ directory\n",
    "env_name = \"gpt4\"\n",
    "load_env(env_name=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82cce2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-03-15-preview'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1992175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-32k\"\n",
    "pivot_lang = \"en\"\n",
    "tgt_lang = \"en\"\n",
    "prompt_name = \"answer_given_context_and_question\"\n",
    "few_shot_k = 4\n",
    "dataset = \"xquad\"\n",
    "short_contexts = False\n",
    "max_tokens = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc5117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkabirahuja2431\u001b[0m (\u001b[33mscai-msri\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/t-kabirahuja/work/repos/MultilingualBlanketEval/wandb/run-20230419_105553-xfa0bgu4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/scai-msri/GPT-4-eval/runs/xfa0bgu4' target=\"_blank\">worldly-haze-42</a></strong> to <a href='https://wandb.ai/scai-msri/GPT-4-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/scai-msri/GPT-4-eval' target=\"_blank\">https://wandb.ai/scai-msri/GPT-4-eval</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/scai-msri/GPT-4-eval/runs/xfa0bgu4' target=\"_blank\">https://wandb.ai/scai-msri/GPT-4-eval/runs/xfa0bgu4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/scai-msri/GPT-4-eval/runs/xfa0bgu4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcd8c47e2b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"model\" : model,\n",
    "    \"pivot_lang\": pivot_lang,\n",
    "    \"tgt_lang\": tgt_lang,\n",
    "    \"prompt_name\": prompt_name,\n",
    "    \"few_shot_k\": few_shot_k,\n",
    "    \"dataset\": dataset,\n",
    "    \"short_contexts\": short_contexts,\n",
    "    \"max_tokens\": max_tokens\n",
    "}\n",
    "\n",
    "wandb.init(project=\"GPT-4-eval\", entity=\"scai-msri\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "645bc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacySentenceTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('xx_ent_wiki_sm')\n",
    "        self.nlp.add_pipe(\"sentencizer\")\n",
    "        \n",
    "    def __call__(self, text: str) -> List[str]:\n",
    "        return list(map(lambda span: span.text, self.nlp(text).sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57a7a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_dataset(dataset_name, lang, split, dataset_frac = 1, translate_test = False):\n",
    "    if dataset_name == \"indicqa\":\n",
    "        if split != \"train\":\n",
    "            dataset = load_dataset(\"ai4bharat/IndicQA\", f\"indicqa.{lang}\")[split]\n",
    "        else:\n",
    "            dataset = load_dataset(\"squad\")[split]\n",
    "    elif dataset_name == \"xquad\":\n",
    "        if split != \"train\":\n",
    "            dataset = load_dataset(\"xquad\", f\"xquad.{lang}\")[split]\n",
    "        else:\n",
    "            dataset = load_dataset(\"squad\")[split]\n",
    "    elif dataset_name == \"tydiqa\":\n",
    "        dataset = load_dataset(\"tydiqa\", 'secondary_task')[split]\n",
    "        dataset = dataset.map(lambda example: {\"lang\" : TYDIQA_LANG2CODES[example[\"id\"].split(\"-\")[0]]})\n",
    "        dataset = dataset.filter(lambda example: example[\"lang\"] == lang)\n",
    "    elif dataset_name == \"mlqa\":\n",
    "        if split == \"train\":\n",
    "            print(\"No Training Data for MLQA, switching to validation!\")\n",
    "            split = \"validation\"\n",
    "        if translate_test:\n",
    "            dataset_name = f\"mlqa-translate-test.{lang}\"\n",
    "        else:\n",
    "            dataset_name = f\"mlqa.{lang}.{lang}\"\n",
    "        \n",
    "        dataset = load_dataset(\"mlqa\", dataset_name)[split]\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    N = len(dataset)\n",
    "    selector = np.arange(int(N * dataset_frac))\n",
    "    return dataset.select(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a81d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/t-kabirahuja/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c9575f181e4846b0582c309daaa7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xquad (/home/t-kabirahuja/.cache/huggingface/datasets/xquad/xquad.en/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5f9f8dbf2e4720a3a9dbc0f2a4ee2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = load_qa_dataset(dataset,\n",
    "                                lang = pivot_lang,\n",
    "                                split=\"train\")\n",
    "test_dataset = load_qa_dataset(dataset,\n",
    "                                lang = tgt_lang,\n",
    "                                split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5782b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if short_contexts:\n",
    "    sent_tokenizer = SpacySentenceTokenizer() \n",
    "\n",
    "    train_dataset = train_dataset.map(lambda example: {\n",
    "        \"context\": [sent for sent in sent_tokenizer(example[\"context\"]) if example[\"answers\"][\"text\"][0] in sent][0]\n",
    "    }, num_proc = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98827b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread SystemMonitor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages/wandb/sdk/internal/system/system_monitor.py\", line 118, in _start\n",
      "    asset.start()\n",
      "  File \"/home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages/wandb/sdk/internal/system/assets/cpu.py\", line 166, in start\n",
      "    self.metrics_monitor.start()\n",
      "  File \"/home/t-kabirahuja/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages/wandb/sdk/internal/system/assets/interfaces.py\", line 168, in start\n",
      "    logger.info(f\"Started {self._process.name}\")\n",
      "AttributeError: 'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "train_examples = choose_few_shot_examples(\n",
    "        train_dataset, few_shot_k, selection_criteria=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b287a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS_DICT = {\n",
    "    \"answer_given_context_and_question\" : \"\"\"{context}\n",
    "    Q: {question}\n",
    "\n",
    "    Referring to the passage above, the correct answer to the given question is:\n",
    "    {answer}\"\"\",\n",
    "    \n",
    "    \"lang_instruct_answer_given_context_and_question\" : \"\"\"{context}\n",
    "    Q: {question}\n",
    "\n",
    "    Referring to the passage above, the correct answer to the given question is? Please try to answer in {language} and ensure that the answer appears as it is in the passage.\n",
    "    A: {answer}\"\"\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45e8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PROMPTS_DICT[prompt_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb47ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.\n"
     ]
    }
   ],
   "source": [
    "# Loading instruction for the task\n",
    "instruction = INSTRUCTIONS[dataset]\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8367f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_metric = load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd344b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Under contract from the U.S. Military, Matrox produced a combination computer/LaserDisc player for instructional purposes. The computer was a 286, the LaserDisc player only capable of reading the analog audio tracks. Together they weighed 43 lb (20 kg) and sturdy handles were provided in case two people were required to lift the unit. The computer controlled the player via a 25-pin serial port at the back of the player and a ribbon cable connected to a proprietary port on the motherboard. Many of these were sold as surplus by the military during the 1990s, often without the controller software. Nevertheless, it is possible to control the unit by removing the ribbon cable and connecting a serial cable directly from the computer's serial port to the port on the LaserDisc player.\\n    Q: How could Matrox's computer unconventionally be controlled?\\n\\n    Referring to the passage above, the correct answer to the given question is:\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"removing the ribbon cable and connecting a serial cable directly from the computer's serial port to the port on the LaserDisc player\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Doctrinal development has had an important place in the restoration of the Preachers. Several institutions, besides those already mentioned, played important parts. Such is the Biblical school at Jerusalem, open to the religious of the order and to secular clerics, which publishes the Revue Biblique. The faculty of theology at the University of Fribourg, confided to the care of the Dominicans in 1890, is flourishing, and has about 250 students. The Pontificium Collegium Internationale Angelicum, the future Pontifical University of Saint Thomas Aquinas, Angelicum established at Rome in 1908 by Master Hyacinth Cormier, opened its doors to regulars and seculars for the study of the sacred sciences. In addition to the reviews above are the Revue Thomiste, founded by Père Thomas Coconnier (d. 1908), and the Analecta Ordinis Prædicatorum (1893). Among numerous writers of the order in this period are: Cardinals Thomas Zigliara (d. 1893) and Zephirin González (d. 1894), two esteemed philosophers; Alberto Guillelmotti (d. 1893), historian of the Pontifical Navy, and Heinrich Denifle, one of the most famous writers on medieval history (d. 1905).[citation needed]\\n    Q: How many students does the University of Fribourg have?\\n\\n    Referring to the passage above, the correct answer to the given question is:'},\n",
       " {'role': 'assistant', 'content': '250'},\n",
       " {'role': 'user',\n",
       "  'content': \"Some scholars note that Tibetan leaders during the Ming frequently engaged in civil war and conducted their own foreign diplomacy with neighboring states such as Nepal. Some scholars underscore the commercial aspect of the Ming-Tibetan relationship, noting the Ming dynasty's shortage of horses for warfare and thus the importance of the horse trade with Tibet. Others argue that the significant religious nature of the relationship of the Ming court with Tibetan lamas is underrepresented in modern scholarship. In hopes of reviving the unique relationship of the earlier Mongol leader Kublai Khan (r. 1260–1294) and his spiritual superior Drogön Chögyal Phagpa (1235–1280) of the Sakya school of Tibetan Buddhism, the Yongle Emperor (r. 1402–1424) made a concerted effort to build a secular and religious alliance with Deshin Shekpa (1384–1415), the Karmapa of the Karma Kagyu school. However, the Yongle Emperor's attempts were unsuccessful.\\n    Q: Who did the Yongle Emperor try to build a religious alliance with?\\n\\n    Referring to the passage above, the correct answer to the given question is:\"},\n",
       " {'role': 'assistant', 'content': 'Deshin Shekpa'},\n",
       " {'role': 'user',\n",
       "  'content': \"Earthworms make a significant contribution to soil fertility. The rear end of the Palolo worm, a marine polychaete that tunnels through coral, detaches in order to spawn at the surface, and the people of Samoa regard these spawning modules as a delicacy. Anglers sometimes find that worms are more effective bait than artificial flies, and worms can be kept for several days in a tin lined with damp moss. Ragworms are commercially important as bait and as food sources for aquaculture, and there have been proposals to farm them in order to reduce over-fishing of their natural populations. Some marine polychaetes' predation on molluscs causes serious losses to fishery and aquaculture operations.\\n    Q: What type of annelid tunnels through coral?\\n\\n    Referring to the passage above, the correct answer to the given question is:\"},\n",
       " {'role': 'assistant', 'content': 'Palolo worm'},\n",
       " {'role': 'user',\n",
       "  'content': 'However, his religious views remain uncertain due to other statements that he made. For example, in his article, \"A Machine to End War\", published in 1937, Tesla stated:\\n    Q: What article was published in 1937?\\n\\n    Referring to the passage above, the correct answer to the given question is:'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = test_dataset[132]\n",
    "\n",
    "prompt, label = construct_qa_prompt(\n",
    "    train_examples,\n",
    "    test_example,\n",
    "    train_prompt_template=prompt_template,\n",
    "    test_prompt_template=prompt_template,\n",
    "    chat_prompt=True,\n",
    "    instruction=instruction\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02ecd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gpt3x_completion(\n",
    "    prompt,\n",
    "    model,\n",
    "    temperature=0,\n",
    "    max_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13e0413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: A Machine to End War\n",
      "Label: \"A Machine to End War\"\n",
      "{'exact_match': 100.0, 'f1': 100.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction: {pred}\")\n",
    "print(f\"Label: {label}\")\n",
    "prediction = {\"prediction_text\": pred, \"id\": test_example[\"id\"]}\n",
    "reference = {}\n",
    "reference[\"answers\"] = test_example[\"answers\"]\n",
    "reference[\"id\"] = test_example[\"id\"]\n",
    "results = squad_metric.compute(\n",
    "            predictions=[prediction],\n",
    "            references=[reference]\n",
    "        )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc555f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "em: 80.0 f1: 91.11111111111109: : 30it [01:36,  2.87s/it]             "
     ]
    }
   ],
   "source": [
    "f1_sum = 0\n",
    "em_sum = 0\n",
    "avg_em = 0\n",
    "avg_f1 = 0\n",
    "\n",
    "run_details = {\"num_calls\": 0}\n",
    "\n",
    "pbar = tqdm(enumerate(test_dataset))\n",
    "\n",
    "for i, test_example in pbar:    \n",
    "    prompt, label = construct_qa_prompt(\n",
    "        train_examples,\n",
    "        test_example,\n",
    "        train_prompt_template=prompt_template,\n",
    "        test_prompt_template=prompt_template,\n",
    "        chat_prompt=True,\n",
    "        instruction=instruction\n",
    "    )\n",
    "    pred = gpt3x_completion(\n",
    "        prompt,\n",
    "        model,\n",
    "        temperature=0,\n",
    "        run_details=run_details,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    prediction = {\"prediction_text\": pred, \"id\": test_example[\"id\"]}\n",
    "    reference = {}\n",
    "    reference[\"answers\"] = test_example[\"answers\"]\n",
    "    reference[\"id\"] = test_example[\"id\"]\n",
    "    results = squad_metric.compute(\n",
    "                predictions=[prediction],\n",
    "                references=[reference])\n",
    "    f1_sum += results[\"f1\"]\n",
    "    em_sum += results[\"exact_match\"]\n",
    "        \n",
    "    avg_f1 = f1_sum / (i+1)\n",
    "    avg_em = em_sum / (i+1)\n",
    "    \n",
    "    wandb.log({\"f1\": avg_f1, \"em\": avg_em})\n",
    "    wandb.log(run_details)\n",
    "    pbar.set_description(f\"em: {avg_em} f1: {avg_f1}\")\n",
    "    time.sleep(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220e468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
