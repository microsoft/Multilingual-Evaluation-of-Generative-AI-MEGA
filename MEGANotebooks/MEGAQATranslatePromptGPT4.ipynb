{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0d19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19a5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "026b1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests, uuid, json\n",
    "from typing import List\n",
    "import spacy\n",
    "import openai\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from mega.data.load_datasets import load_xnli_dataset\n",
    "from mega.data.data_utils import choose_few_shot_examples\n",
    "from mega.prompting.instructions import INSTRUCTIONS\n",
    "from mega.prompting.prompting_utils import load_prompt_template\n",
    "from mega.utils.env_utils import load_env\n",
    "from mega.models.completion_models import get_model_pred, gpt3x_completion\n",
    "from mega.prompting.prompting_utils import construct_prompt, construct_qa_prompt\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "\n",
    "# Translator setup for bing\n",
    "endpoint = \"https://api.cognitive.microsofttranslator.com/\"\n",
    "with open(\"keys/bing_translate_key.txt\") as f:\n",
    "    subscription_key = f.read().split(\"\\n\")[0]\n",
    "    \n",
    "# Add your location, also known as region. The default is global.\n",
    "# This is required if using a Cognitive Services resource.\n",
    "location = \"centralindia\"\n",
    "path = \"/translate?api-version=3.0\"\n",
    "constructed_url = endpoint + path\n",
    "\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\": subscription_key,\n",
    "    \"Content-type\": \"application/json\",\n",
    "    \"X-ClientTraceId\": str(uuid.uuid4()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ef56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that {env_name}.env file is present in the envs/ directory\n",
    "env_name = \"gpt4\"\n",
    "load_env(env_name=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cce2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-03-15-preview'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1992175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-32k\"\n",
    "pivot_lang = \"ko\"\n",
    "tgt_lang = \"ko\"\n",
    "prompt_name = \"answer_given_context_and_question\"\n",
    "few_shot_k = 4\n",
    "dataset = \"tydiqa\"\n",
    "short_contexts = False\n",
    "max_tokens = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0fc5117e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\" : model,\n",
    "    \"pivot_lang\": pivot_lang,\n",
    "    \"tgt_lang\": tgt_lang,\n",
    "    \"prompt_name\": prompt_name,\n",
    "    \"few_shot_k\": few_shot_k,\n",
    "    \"dataset\": dataset,\n",
    "    \"short_contexts\": short_contexts,\n",
    "    \"max_tokens\": max_tokens\n",
    "}\n",
    "\n",
    "# wandb.init(project=\"GPT-4-eval\", entity=\"scai-msri\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "645bc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacySentenceTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('xx_ent_wiki_sm')\n",
    "        self.nlp.add_pipe(\"sentencizer\")\n",
    "        \n",
    "    def __call__(self, text: str) -> List[str]:\n",
    "        return list(map(lambda span: span.text, self.nlp(text).sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57a7a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_dataset(dataset_name, lang, split, dataset_frac = 1, translate_test = False):\n",
    "    if dataset_name == \"indicqa\":\n",
    "        if split != \"train\":\n",
    "            dataset = load_dataset(\"ai4bharat/IndicQA\", f\"indicqa.{lang}\")[split]\n",
    "        else:\n",
    "            dataset = load_dataset(\"squad\")[split]\n",
    "    elif dataset_name == \"xquad\":\n",
    "        if split != \"train\":\n",
    "            dataset = load_dataset(\"xquad\", f\"xquad.{lang}\")[split]\n",
    "        else:\n",
    "            dataset = load_dataset(\"squad\")[split]\n",
    "    elif dataset_name == \"tydiqa\":\n",
    "        dataset = load_dataset(\"tydiqa\", 'secondary_task')[split]\n",
    "        dataset = dataset.map(lambda example: {\"lang\" : TYDIQA_LANG2CODES[example[\"id\"].split(\"-\")[0]]})\n",
    "        dataset = dataset.filter(lambda example: example[\"lang\"] == lang)\n",
    "    elif dataset_name == \"mlqa\":\n",
    "        if split == \"train\":\n",
    "            print(\"No Training Data for MLQA, switching to validation!\")\n",
    "            split = \"validation\"\n",
    "        if translate_test:\n",
    "            dataset_name = f\"mlqa-translate-test.{lang}\"\n",
    "        else:\n",
    "            dataset_name = f\"mlqa.{lang}.{lang}\"\n",
    "        \n",
    "        dataset = load_dataset(\"mlqa\", dataset_name)[split]\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    N = len(dataset)\n",
    "    selector = np.arange(int(N * dataset_frac))\n",
    "    return dataset.select(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edc89fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYDIQA_LANG2CODES = {\n",
    "    \"bengali\": \"bn\",\n",
    "    \"korean\" : \"ko\",\n",
    "    \"swahili\" : \"sw\",\n",
    "    \"english\" : \"en\",\n",
    "    \"indonesian\" :\"id\",\n",
    "    \"arabic\" : \"ar\",\n",
    "    \"finnish\" : \"fi\",\n",
    "    \"telugu\" : \"te\",\n",
    "    \"russian\" : \"ru\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a527f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LangCodes2LangNames = {\n",
    "    \"en\" : \"English\",\n",
    "    \"ko\" : \"Korean\",\n",
    "    \"es\" : \"Spanish\",\n",
    "    \"de\" : \"German\",\n",
    "    \"fr\" : \"French\",\n",
    "    \"zh\" : \"Chinese\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a81d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tydiqa (/home/t-kabirahuja/.cache/huggingface/datasets/tydiqa/secondary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacd47467470458e8f1d39927aacd91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/t-kabirahuja/.cache/huggingface/datasets/tydiqa/secondary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-e999b14bc35f2874.arrow\n",
      "Loading cached processed dataset at /home/t-kabirahuja/.cache/huggingface/datasets/tydiqa/secondary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-f1e0547ce5848305.arrow\n",
      "Found cached dataset tydiqa (/home/t-kabirahuja/.cache/huggingface/datasets/tydiqa/secondary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be1f5bab78c4617aa3dec97d8210c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/t-kabirahuja/.cache/huggingface/datasets/tydiqa/secondary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-9154e010a5c45085.arrow\n",
      "Loading cached processed dataset at /home/t-kabirahuja/.cache/huggingface/datasets/tydiqa/secondary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-b2cf73e837a00e99.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_qa_dataset(dataset,\n",
    "                                lang = pivot_lang,\n",
    "                                split=\"train\")\n",
    "test_dataset = load_qa_dataset(dataset,\n",
    "                                lang = tgt_lang,\n",
    "                                split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5782b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if short_contexts:\n",
    "    sent_tokenizer = SpacySentenceTokenizer() \n",
    "\n",
    "    train_dataset = train_dataset.map(lambda example: {\n",
    "        \"context\": [sent for sent in sent_tokenizer(example[\"context\"]) if example[\"answers\"][\"text\"][0] in sent][0]\n",
    "    }, num_proc = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98827b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = choose_few_shot_examples(\n",
    "        train_dataset, few_shot_k, selection_criteria=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b287a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS_DICT = {\n",
    "    \"answer_given_context_and_question\" : \"\"\"{context}\n",
    "    Q: {question}\n",
    "\n",
    "    Referring to the passage above, the correct answer to the given question is:\n",
    "    {answer}\"\"\",\n",
    "    \n",
    "    \"lang_instruct_answer_given_context_and_question\" : \"\"\"{context}\n",
    "    Q: {question}\n",
    "\n",
    "    Referring to the passage above, the correct answer to the given question is? Please try to answer in {language} and ensure that the answer appears as it is in the passage.\n",
    "    A: {answer}\"\"\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a45e8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PROMPTS_DICT[prompt_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1eb47ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.\n"
     ]
    }
   ],
   "source": [
    "# Loading instruction for the task\n",
    "instruction = INSTRUCTIONS[dataset]\n",
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71060cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'system',\n",
       " 'content': 'You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_prompt = {\"role\": \"system\", \"content\": instruction}\n",
    "instruction_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b30de756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_with_bing(text: str, src: str, dest: str) -> str:\n",
    "    \"\"\"Uses the bing translator to translate `text` from `src` language to `dest` language\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to translate\n",
    "        src (str): Source language to translate from\n",
    "        dest (str): Language to translate to\n",
    "\n",
    "    Returns:\n",
    "        str: Translated text\n",
    "    \"\"\"\n",
    "    params = {\"api-version\": \"3.0\", \"from\": src, \"to\": [dest]}\n",
    "    body = [{\"text\": text}]\n",
    "\n",
    "    try:\n",
    "        request = requests.post(\n",
    "            constructed_url, params=params, headers=headers, json=body\n",
    "        )\n",
    "        response = request.json()\n",
    "        # pdb.set_trace()\n",
    "        translation = response[0][\"translations\"][0][\"text\"]\n",
    "    except:\n",
    "        pdb.set_trace()\n",
    "        translation = \"<MT Failed>\"\n",
    "\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a85e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_template(template, example, fill_answer=True):\n",
    "    if fill_answer:\n",
    "        return template.replace(\"{context}\", example[\"context\"])\\\n",
    "                .replace(\"{question}\", example[\"question\"])\\\n",
    "                .replace(\"{answer}\", example[\"answers\"][\"text\"][0])\n",
    "    else:\n",
    "        return template.replace(\"{context}\", example[\"context\"])\\\n",
    "                .replace(\"{question}\", example[\"question\"])\\\n",
    "                .replace(\"{answer}\", \"\").strip()\n",
    "    \n",
    "def construct_translate_cot_prompt(example, lang, test=False):\n",
    "        \n",
    "    answer_step_by_step_format = \"Let's do it step by step.\"\n",
    "    answer_passage_trans_format = \"Translation of the passage in English is: \\\"{translated_en_passage}\\\"\"\n",
    "    answer_question_trans_format = \"Translation of the question in English is: \\\"{translated_en_question}\\\"\"\n",
    "    answer_en_answer_format = \"Then the answer in English will be: {answer_en}\"\n",
    "    answer_format = \"Hence the final answer in {lang} is: {answer}\"\n",
    "    \n",
    "    \n",
    "    user_prompt = {\"role\": \"user\", \"content\": fill_template(prompt_template, example, fill_answer=False)}\n",
    "    if test:\n",
    "        return [user_prompt]\n",
    "    \n",
    "    assistant_answers = [\n",
    "        answer_step_by_step_format,\n",
    "        answer_passage_trans_format.replace(\"{translated_en_passage}\", translate_with_bing(example[\"context\"], src = lang, dest = \"en\")),\n",
    "        answer_question_trans_format.replace(\"{translated_en_question}\", translate_with_bing(example[\"question\"], src = lang, dest = \"en\")),\n",
    "        answer_en_answer_format.replace(\"{answer_en}\", translate_with_bing(example[\"answers\"][\"text\"][0], src = lang, dest = \"en\")),\n",
    "        answer_format.replace(\"{answer}\", example[\"answers\"][\"text\"][0]).replace(\"{lang}\", LangCodes2LangNames[lang])\n",
    "    ]\n",
    "\n",
    "    assistant_answers = [{\"role\": \"assistant\", \"content\": answer} for answer in assistant_answers]\n",
    "    return [user_prompt] +  assistant_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cd1579ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['예정'], 'answer_start': [59]}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "274025ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'korean--5252295675424886259-0',\n",
       " 'title': '2019년 FIFA U-20 월드컵',\n",
       " 'context': '2019년 FIFA U-20 월드컵</b>은 2019년 5월 23일부터 6월 15일까지 폴란드에서 개최될 예정인 22번째 FIFA U-20 월드컵이다.',\n",
       " 'question': '2019년까지 월드컵은 몇개국에서 개최되었는가?',\n",
       " 'answers': {'text': ['예정'], 'answer_start': [59]},\n",
       " 'lang': 'ko'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "608298be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '연구방식은 크게 두 가지로 나눌 수 있다. 이는 탑다운(Top-down)방식과 바텀업(Bottom-up)방식이다. 탑다운 방식은 자연계에 존재하는 생명체의 유전자를 변형시키는 방식이다. 하나의 세포로 이뤄진 미생물을 예로 들면, 미생물의 유전자 일부를 바꾸는 것이다. 미국 스탠포드대에서 각각의 유전자를 기계의 부품처럼 만들어 다양하게 조합한 후 미생물에 삽입을 시도하고 있는 드루앤디(Drew Endy)의 접근이 한 가지 사례이다. 또 미국 UC버클리의 제이 키슬링(Jay Keasling)이 식물의 유용 유전자를 미생물에 대량으로 삽입하고 있는 것도 이에 해당한다. 이에 비해 바텀업 방식은 화학물질에서 시작해 생명체의 구성요소를 하나하나 만들어 내는 방식이다. 그래서 합성생물학 관련 영문저술에는 종종 ‘무에서 유를 창조한다’는 의미에서 ‘Starting from scratch’라는 즉, ‘처음에서 시작한다’는 뜻의 관용어구가 등장한다. 미생물의 유전자 염기서열을 하나하나를 만든 후 이들을 연결시켜 인공유전체를 합성한 미국의 크레이그 밴터(J.Craig Venter)의 접근이 대표사례이다.\\n    Q: 합성생물학을 연구하는 방식은 탑다운 외 다른 방식은 무엇이 있나요?\\n\\n    Referring to the passage above, the correct answer to the given question is:'},\n",
       " {'role': 'assistant', 'content': \"Let's do it step by step.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the passage in English is: \"There are two main types of research methods. This is a top-down and bottom-up method. The top-down method is a method of modifying the genes of living beings in nature. For example, a microorganism made up of a single cell changes some of its genes. One example is Drew Endy\\'s approach at Stanford University in the United States, where each gene is being made like a part of a machine, combined in various ways, and then inserted into a microorganism. This is also the case with UC Berkeley\\'s Jay Keasling, who inserts useful genes from plants into microorganisms in large quantities. In comparison, the bottom-up method starts from chemicals and creates the components of living things one by one. Therefore, in English writings related to synthetic biology, the idiom \"Starting from scratch\" often appears in the sense of \"creating something out of nothing,\" that is, \"starting from the beginning.\" A case in point is the approach of J.Craig Venter of the United States, who created the gene sequences of microorganisms one by one and then linked them to synthesize the whole human body.\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the question in English is: \"What are some ways to study synthetic biology other than top-down?\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Then the answer in English will be: Bottom Up'},\n",
       " {'role': 'assistant', 'content': 'Hence the final answer in Korean is: 바텀업'}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_translate_cot_prompt(\n",
    "    train_dataset[1],\n",
    "    lang = tgt_lang\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "466959d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompts = []\n",
    "for example in train_examples:\n",
    "    few_shot_prompts += construct_translate_cot_prompt(example, lang = tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a805ade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Then the answer in English will be: 30 December 1922'}\n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b8367f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_metric = load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7fd344b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an NLP assistant whose purpose is to solve reading comprehension problems. You will be provided questions on a set of passages and you will need to provide the answer as it appears in the passage. The answer should be in the same language as the question and the passage.'},\n",
       " {'role': 'user',\n",
       "  'content': '소비에트 사회주의 공화국 연방(, ), 약칭 소비에트 연방() 또는 소련(蘇聯)은 1922년 12월 30일부터 1991년 12월 26일까지 유라시아 북부에 존재하였던 세계 최초의 공산주의 국가였다.\\n    Q: 소련의 설립일은 언제인가요?\\n\\n    Referring to the passage above, the correct answer to the given question is:'},\n",
       " {'role': 'assistant', 'content': \"Let's do it step by step.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the passage in English is: \"The Union of Soviet Socialist Republics (, ), abbreviated Soviet Union (), or Soviet Union (蘇聯) was the world\\'s first communist state that existed in northern Eurasia from December 30, 1922, to December 26, 1991.\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the question in English is: \"When was the Soviet Union founded?\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Then the answer in English will be: 30 December 1922'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hence the final answer in Korean is: 1922년 12월 30일'},\n",
       " {'role': 'user',\n",
       "  'content': '2002년에 13세살인 보컬 헤일리 윌리엄스는 고향인 미시시피 주 메리디엔에서 테네시 주 프랭클린으로 이사갔다. 그 곳에서 그녀는 사립학교에 다니면서 패로 형제(조시 패로, 잭 패로)를 만났다. 떠나기에 앞서 그녀는 브렛 매닝에게 보컬 수업을 받았다. 파라모어 결성에 앞서, 윌리엄과 베이시스트 제레미는 패로형제가 학교끝나고 연습할 동안에 친구 키미 리드와 함께 The Factory로 불리는 펑크 커버 밴드에 참여했다. 파라모어가 되기 직전에 다른 멤버들은 보컬로서 헤일리가 갖는 \"여성이라는 초조함\"이 있었지만 그들은 좋은 친구였고 그녀는 후에 합류한 그녀의 이웃인 제이슨 바이넘 (리듬 기타)과 함께 작사하기 시작했다. 헤일리의 말에 따르면, 밴드 명인 \"Paramore\"는 그들의 처음 베이시스트 였던 이의 엄마의 독신 이름이었다고 한다. 밴드가 그 이름의 동의어인 paramour(\"secret lover: 숨겨둔 사랑\")의 뜻을 알고나서 \"Paramore\"라고 쓴채로 그 의미를 채택했다.\\n    Q: 파리모어의 리드 보컬 고향은 미국에서 어디인가요?\\n\\n    Referring to the passage above, the correct answer to the given question is:'},\n",
       " {'role': 'assistant', 'content': \"Let's do it step by step.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the passage in English is: \"In 2002, 13-year-old vocalist Hayley Williams moved from her hometown of Meridien, Mississippi, to Franklin, Tennessee. There, while attending a private school, she met the Farrow brothers (Josh Farrow and Jack Farrow). Before leaving, she took vocal lessons from Brett Manning. Prior to forming Paramore, William and bassist Jeremy joined a punk cover band called The Factory with their friend Kimmy Reed while the Farrow brothers practiced after school. Shortly before Paramore, the other members had Haley\\'s \"feminine nervousness\" as vocalists, but they were good friends and she started writing with her neighbour Jason Bynum (rhythm guitar), who later joined. According to Haley, the band\\'s name, \"Paramore,\" was the single name of their first bassist\\'s mother. When the band learned the meaning of the name\\'s synonym, paramour (\"secret lover\"), they adopted it by writing \"Paramore\".\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the question in English is: \"Where is Farimore\\'s lead vocal hometown in the United States?\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Then the answer in English will be: Meridien, Mississippi'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hence the final answer in Korean is: 미시시피 주 메리디엔'},\n",
       " {'role': 'user',\n",
       "  'content': '홍콩(Chinese:香港; pinyin:Xiānggǎng, , ), 혹은 중화인민공화국 홍콩 특별행정구(simplified Chinese:中华人民共和国香港特别行政区; traditional Chinese:中華人民共和國香港特別行政區; pinyin:Zhōnghuá Rénmín Gònghéguó Xiānggǎng Tèbié Xíngzhèngqū, , )는 중화인민공화국 화난 주강 삼각강 동쪽에 위치한 자치행정구역이다. 홍콩은 마카오, 광저우, 선전, 주하이, 그리고 광둥성의 다른 주요 도시들과 함께 세계에서 가장 인구가 많은 주강 삼각주 대도시권을 형성하고 있다. 홍콩은 1,104km2의 면적에 740만 이상의 다양한 국적 출신의 홍콩인들로 구성, 세계에서 4번째로 높은 인구밀도 지역이다.\\n    Q: 홍콩의 면적은 얼마나 되나요?\\n\\n    Referring to the passage above, the correct answer to the given question is:'},\n",
       " {'role': 'assistant', 'content': \"Let's do it step by step.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the passage in English is: \"Hong Kong (Chinese:香港; pinyin:Xiānggǎng, , ), or Hong Kong Special Administrative Region of the People\\'s Republic of China (simplified Chinese:中华人民共和国香港特别行政区; traditional Chinese:中華人民共和國香港特別行政區; pinyin:Zhōnghuá rénmín Gònghéguó Xiānggǎng Tèbié Xíngzhèngqū, , ) is an autonomous administrative region located east of the Pearl River Delta River in Huanan in the People\\'s Republic of China. Hong Kong, along with Macau, Guangzhou, Shenzhen, Zhuhai and other major cities in Guangdong province, forms the world\\'s most populous Pearl River Delta metropolitan area. Hong Kong covers an area of 1,104 square kilometers and is home to more than 7.4 million Hong Kongers of various nationalities, making it the fourth most densely populated area in the world.\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the question in English is: \"How big is Hong Kong?\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Then the answer in English will be: 1,104km2'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hence the final answer in Korean is: 1,104km2'},\n",
       " {'role': 'user',\n",
       "  'content': '1945년 8월 15일 제2차 세계대전에서 일본이 패망하자 한국은 해방을 맞았다. 해방부터 대한민국과 조선민주주의인민공화국이 성립되는 시기까지를 군정기 또는 해방정국이라 한다. 해방정국에서 남과 북은 극심한 좌우 갈등을 겪었다. 북에서는 조만식과 같은 우익 인사에 대한 탄압이 있었고, 남에서는 여운형과 같은 중도 좌파 정치인이 암살되었다. 국제사회에서는 모스크바 3상회의를 통해 소련과 미국에서 통일 임시정부 수립을 위한 신탁통치를 계획했지만, 한국에서의 극심한 반대와 함께 미소공동위원회의 결렬로 폐기되었다. 1948년 대한민국과 조선민주주의인민공화국의 정부가 별도로 수립되면서 일본군의 무장해제를 위한 군사분계선이었던 38선은 두 개의 국가로 분단되는 기준이 되었다. 분단 이후 양측 간의 긴장이 이어졌고 수시로 국지적인 교전이 있었다. 1950년 6월 25일 조선인민군이 일제히 38선을 넘어 대한민국을 침략하여 한국 전쟁이 발발하였다.\\n    Q: 대한민국은 언제 독립하였는가?\\n\\n    Referring to the passage above, the correct answer to the given question is:'},\n",
       " {'role': 'assistant', 'content': \"Let's do it step by step.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the passage in English is: \"Following Japan\\'s defeat in World War II on August 15, 1945, Korea was liberated. The period from the liberation to the establishment of the Republic of Korea and the Democratic People\\'s Republic of Korea is called the military period or liberation government. In the liberation government, the North and the South experienced a fierce left-right conflict. In the North, there was a crackdown on right-wing figures such as Cho Man-sik, and in the South, center-left politicians such as Yeo Woon-hyung were assassinated. The international community planned a trusteeship for the establishment of a unified provisional government in the Soviet Union and the United States through the Moscow Trilateral Conference, but it was scrapped due to the breakdown of the Joint Committee on Micronations due to fierce opposition in Korea. With the separate establishment of the governments of the Republic of Korea and the Democratic People\\'s Republic of Korea in 1948, the 38th parallel, which was the military demarcation line for the disarmament of the Japanese army, became the basis for the division into two countries. After the division, tensions between the two sides followed, with occasional localized skirmishes. On June 25, 1950, the Korean People\\'s Army crossed the 38th parallel in unison and invaded the Republic of Korea, and the Korean War broke out.\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Translation of the question in English is: \"When did the Republic of Korea become independent?\"'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Then the answer in English will be: 15 Aug 1945'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hence the final answer in Korean is: 1945년 8월 15일'},\n",
       " {'role': 'user',\n",
       "  'content': '캐나다(, , )는 북아메리카 북부의 연방 국가이다. 수도는 오타와이며, 최대도시는 토론토이다. 동쪽에는 대서양, 서쪽에는 태평양, 북쪽에는 북극해가 접해있다. 전 세계 국가 중 러시아에 이어 국토 면적이 두 번째로 크며, 미국과의 국경은 두 나라간 국경 중 세계에서 가장 긴 국경이다. 10개의 주와 3개의 준주로 구성되어 있다.\\n    Q: 캐나다의 현재 수도는 어디인가요?\\n\\n    Referring to the passage above, the correct answer to the given question is:'}]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = test_dataset[132]\n",
    "\n",
    "\n",
    "test_prompt = construct_translate_cot_prompt(test_example, tgt_lang, test=True)\n",
    "\n",
    "prompt = [instruction_prompt] + few_shot_prompts + test_prompt\n",
    "label = test_example[\"answers\"][\"text\"][0]\n",
    "\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "70b85ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "02ecd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for _ in range(20):\n",
    "    \n",
    "    pred = gpt3x_completion(\n",
    "        prompt,\n",
    "        model,\n",
    "        temperature=0,\n",
    "        max_tokens = 100\n",
    "    )\n",
    "    prompt += [{\"role\": \"assistant\", \"content\": pred}]\n",
    "    preds.append(pred)\n",
    "    if \"Hence the final answer\" in pred:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "02545b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hence the final answer in Korean is: 오타와']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "75a88663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "66d3dd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오타와'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = preds[-1].split(\":\")[-1].strip()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "13e0413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 오타와\n",
      "Label: 오타와\n",
      "{'exact_match': 100.0, 'f1': 100.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction: {pred}\")\n",
    "print(f\"Label: {label}\")\n",
    "prediction = {\"prediction_text\": pred, \"id\": test_example[\"id\"]}\n",
    "reference = {}\n",
    "reference[\"answers\"] = test_example[\"answers\"]\n",
    "reference[\"id\"] = test_example[\"id\"]\n",
    "results = squad_metric.compute(\n",
    "            predictions=[prediction],\n",
    "            references=[reference]\n",
    "        )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bc555f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "em: 50.0 f1: 65.17197331387709: : 160it [1:59:08, 45.14s/it]               /home/t-kabirahuja/work/repos/MultilingualBlanketEval/mega/models/completion_models.py:98: UserWarning: Couldn't generate response, returning empty string as response\n",
      "  warnings.warn(\n",
      "em: 50.84033613445378 f1: 62.88800698400471: : 238it [2:57:23, 44.72s/it]  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/t-kabirahuja/.cache/huggingface/metrics/squad/default'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m reference[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m reference[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msquad_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m f1_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     48\u001b[0m em_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexact_match\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages/evaluate/module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages/evaluate/module.py:481\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages/evaluate/module.py:605\u001b[0m, in \u001b[0;36mEvaluationModule._init_writer\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# Get cache file name and lock it\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     cache_file_name, filelock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_cache_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# get ready\u001b[39;00m\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m cache_file_name\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelock \u001b[38;5;241m=\u001b[39m filelock\n",
      "File \u001b[0;32m~/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages/evaluate/module.py:268\u001b[0m, in \u001b[0;36mEvaluationModule._create_cache_file\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    266\u001b[0m filelock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_concurrent_cache_files):\n\u001b[0;32m--> 268\u001b[0m     filelock \u001b[38;5;241m=\u001b[39m \u001b[43mFileLock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.lock\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m         filelock\u001b[38;5;241m.\u001b[39macquire(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/work/repos/MultilingualBlanketEval/envs/mega2env/lib/python3.8/site-packages/datasets/utils/filelock.py:399\u001b[0m, in \u001b[0;36mUnixFileLock.__init__\u001b[0;34m(self, lock_file, timeout, max_filename_length)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lock_file, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, max_filename_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 399\u001b[0m     max_filename_length \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatvfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mf_namemax\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(lock_file, timeout\u001b[38;5;241m=\u001b[39mtimeout, max_filename_length\u001b[38;5;241m=\u001b[39mmax_filename_length)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/t-kabirahuja/.cache/huggingface/metrics/squad/default'"
     ]
    }
   ],
   "source": [
    "f1_sum = 0\n",
    "em_sum = 0\n",
    "avg_em = 0\n",
    "avg_f1 = 0\n",
    "\n",
    "run_details = {\"num_calls\": 0}\n",
    "\n",
    "pbar = tqdm(enumerate(test_dataset))\n",
    "\n",
    "for i, test_example in pbar:    \n",
    "    test_prompt = construct_translate_cot_prompt(test_example, tgt_lang, test=True)\n",
    "\n",
    "    prompt = [instruction_prompt] + few_shot_prompts + test_prompt\n",
    "    label = test_example[\"answers\"][\"text\"][0]\n",
    "    preds = []\n",
    "    for _ in range(20):\n",
    "        try:\n",
    "            pred = gpt3x_completion(\n",
    "                prompt,\n",
    "                model,\n",
    "                temperature=0,\n",
    "                evals_per_second=2,\n",
    "                run_details=run_details,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            prompt += [{\"role\": \"assistant\", \"content\": pred}]\n",
    "            preds.append(pred)\n",
    "            if \"Hence the final answer\" in pred:\n",
    "                break\n",
    "        except:\n",
    "            pred = \"\" if preds == [] else pred\n",
    "            break\n",
    "            \n",
    "    if \"Hence the final answer\" in pred:\n",
    "        answer_pred = preds[-1].split(\":\")[-1].strip()\n",
    "    else:\n",
    "        answer_pred = \"\"\n",
    "    \n",
    "    \n",
    "    prediction = {\"prediction_text\": answer_pred, \"id\": test_example[\"id\"]}\n",
    "    reference = {}\n",
    "    reference[\"answers\"] = test_example[\"answers\"]\n",
    "    reference[\"id\"] = test_example[\"id\"]\n",
    "    results = squad_metric.compute(\n",
    "                predictions=[prediction],\n",
    "                references=[reference])\n",
    "    f1_sum += results[\"f1\"]\n",
    "    em_sum += results[\"exact_match\"]\n",
    "        \n",
    "    avg_f1 = f1_sum / (i+1)\n",
    "    avg_em = em_sum / (i+1)\n",
    "    \n",
    "#     wandb.log({\"f1\": avg_f1, \"em\": avg_em}, step = i+1)\n",
    "#     wandb.log(run_details, step = i+1)\n",
    "    pbar.set_description(f\"em: {avg_em} f1: {avg_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf012e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
