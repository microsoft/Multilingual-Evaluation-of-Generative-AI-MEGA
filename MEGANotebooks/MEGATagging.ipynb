{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "8b0d19e8",
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload``\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "a19a5cff",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.chdir(\"../\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "60caae85",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pwd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "026b1300",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/anaconda/envs/mega/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.7.3) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
                        "  warnings.warn(\n",
                        "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "import random\n",
                "from typing import List\n",
                "import spacy\n",
                "import openai\n",
                "import numpy as np\n",
                "import wandb\n",
                "from datasets import load_dataset\n",
                "from mega.data.load_datasets import load_xnli_dataset\n",
                "from mega.data.data_utils import choose_few_shot_examples\n",
                "from mega.prompting.instructions import INSTRUCTIONS\n",
                "from mega.prompting.prompting_utils import load_prompt_template\n",
                "from mega.utils.env_utils import load_openai_env_variables\n",
                "# from mega.models.completion_models import get_model_pred, gpt3x_completion\n",
                "from mega.models.tag_models import gpt3x_tagger\n",
                "from mega.models.completion_models import gpt3x_completion\n",
                "from mega.prompting.prompting_utils import construct_prompt, construct_qa_prompt, construct_tagging_prompt\n",
                "from mega.data.load_datasets import load_tagging_dataset\n",
                "from seqeval.metrics import f1_score\n",
                "from tqdm.notebook import tqdm\n",
                "from evaluate import load\n",
                "\n",
                "# Set seed\n",
                "random.seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "73ef56fb",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Make sure that {env_name}.env file is present in the envs/ directory\n",
                "from dotenv import load_dotenv\n",
                "env_name = \"melange\"\n",
                "# load_env(env_name=env_name)\n",
                "load_dotenv(\"envs/melange.env\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82cce2c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "openai.api_version = \"2023-03-15-preview\"\n",
                "openai.api_version"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1059d0c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "openai.api_base"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "1992175f",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = \"gpt-35-turbo\"\n",
                "pivot_lang = \"fr\"\n",
                "tgt_lang = \"fr\"\n",
                "prompt_name = \"structure_prompting_chat\"\n",
                "few_shot_k = 8\n",
                "dataset = \"udpos\"\n",
                "# short_contexts = False\n",
                "max_tokens = 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "0fc5117e",
            "metadata": {},
            "outputs": [],
            "source": [
                "config = {\n",
                "    \"model\" : model,\n",
                "    \"pivot_lang\": pivot_lang,\n",
                "    \"tgt_lang\": tgt_lang,\n",
                "    \"prompt_name\": prompt_name,\n",
                "    \"few_shot_k\": few_shot_k,\n",
                "    \"dataset\": dataset,\n",
                "    \"max_tokens\": max_tokens\n",
                "}\n",
                "\n",
                "# wandb.init(project=\"GPT-4-eval\", entity=\"scai-msri\", config=config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "645bc52f",
            "metadata": {},
            "outputs": [],
            "source": [
                "class SpacySentenceTokenizer:\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.nlp = spacy.load('xx_ent_wiki_sm')\n",
                "        self.nlp.add_pipe(\"sentencizer\")\n",
                "        \n",
                "    def __call__(self, text: str) -> List[str]:\n",
                "        return list(map(lambda span: span.text, self.nlp(text).sents))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "57a7a04d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def load_tagging_dataset(\n",
                "#     dataset: str,\n",
                "#     lang: str,\n",
                "#     split: str,\n",
                "#     dataset_frac: float = 1.0,\n",
                "#     xtreme_dir: str = \"xtreme/download\",\n",
                "#     delimiter: str = \"_\",\n",
                "# ):\n",
                "\n",
                "#     split = \"dev\" if split == \"validation\" else split\n",
                "\n",
                "#     filename = f\"{xtreme_dir}/{dataset}/{split}-{lang}.tsv\"\n",
                "#     inputs, labels = read_conll_data(filename)\n",
                "\n",
                "#     dataset = Dataset.from_dict({\"tokens\": inputs, \"tags\": labels})\n",
                "#     dataset = dataset.map(\n",
                "#         lambda example: {\n",
                "#             \"tagged_tokens\": [f\"{token}{delimiter}{tag}\"\n",
                "#             for token, tag in zip(example[\"tokens\"], example[\"tags\"])]\n",
                "#         }\n",
                "#     )\n",
                "\n",
                "#     N = len(dataset)\n",
                "#     selector = np.arange(int(N * dataset_frac))\n",
                "#     return dataset.select(selector)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "6a81d928",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map:  46%|████▌     | 1825/3973 [00:00<00:00, 18149.41 examples/s]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map: 100%|██████████| 3973/3973 [00:00<00:00, 16121.98 examples/s]\n"
                    ]
                }
            ],
            "source": [
                "train_dataset = load_tagging_dataset(dataset,\n",
                "                                lang = pivot_lang,\n",
                "                                split=\"dev\")\n",
                "# test_dataset = load_tagging_dataset(dataset,\n",
                "#                                 lang = tgt_lang,\n",
                "#                                 split=\"test\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "e2c73368",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'tokens': ['From', 'the', 'AP', 'comes', 'this', 'story', ':'],\n",
                            " 'tags': ['ADP', 'DET', 'PROPN', 'VERB', 'DET', 'NOUN', 'PUNCT'],\n",
                            " 'tagged_tokens': ['From_ADP',\n",
                            "  'the_DET',\n",
                            "  'AP_PROPN',\n",
                            "  'comes_VERB',\n",
                            "  'this_DET',\n",
                            "  'story_NOUN',\n",
                            "  ':_PUNCT']}"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_dataset[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "98827b9b",
            "metadata": {},
            "outputs": [],
            "source": [
                "train_examples = choose_few_shot_examples(\n",
                "        train_dataset, few_shot_k, selection_criteria=\"random\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "b287a181",
            "metadata": {},
            "outputs": [],
            "source": [
                "PROMPTS_DICT = {\n",
                "    \"structure_prompting\": \"\"\"C: {context}\\nT: {tagged}\"\"\",\n",
                "    \"structure_prompting_chat\": \"\"\"Tag the following sentence: \"{context}\"\\n{tagged}\"\"\"\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "a45e8d1f",
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_template = PROMPTS_DICT[prompt_name]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "1eb47ad6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "You are an NLP assistant whose purpose is to perform Part of Speech (PoS) Tagging. PoS tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. You will need to use the tags defined below:\n",
                        "    1. ADJ: adjective\n",
                        "    2. ADP: adposition\n",
                        "    3. ADV: adverb\n",
                        "    4. AUX: auxiliary\n",
                        "    5. CCONJ: coordinating-conjunction\n",
                        "    6. DET: determiner\n",
                        "    7. INTJ: interjection\n",
                        "    8. NOUN: noun\n",
                        "    9. NUM: numeral\n",
                        "    10. PART: particle\n",
                        "    11. PRON: pronoun\n",
                        "    12. PROPN: proper-noun\n",
                        "    13. PUNCT: punctuation\n",
                        "    14. SCONJ: subordinating-conjunction\n",
                        "    15. SYM: symbol\n",
                        "    16. VERB: verb\n",
                        "    17. X: other\n",
                        "    Do not try to answer the question! Just tag each token in the sentence.\n"
                    ]
                }
            ],
            "source": [
                "# Loading instruction for the task\n",
                "instruction = INSTRUCTIONS[dataset]\n",
                "print(instruction)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "25c867f1",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[{'tokens': ['In',\n",
                            "   '1599',\n",
                            "   ',',\n",
                            "   'a',\n",
                            "   'partnership',\n",
                            "   'of',\n",
                            "   'company',\n",
                            "   'members',\n",
                            "   'built',\n",
                            "   'their',\n",
                            "   'own',\n",
                            "   'theatre',\n",
                            "   'on',\n",
                            "   'the',\n",
                            "   'south',\n",
                            "   'bank',\n",
                            "   'of',\n",
                            "   'the',\n",
                            "   'River',\n",
                            "   'Thames',\n",
                            "   ',',\n",
                            "   'which',\n",
                            "   'they',\n",
                            "   'called',\n",
                            "   'the',\n",
                            "   'Globe',\n",
                            "   '.'],\n",
                            "  'tags': ['ADP',\n",
                            "   'NUM',\n",
                            "   'PUNCT',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'ADP',\n",
                            "   'NOUN',\n",
                            "   'NOUN',\n",
                            "   'VERB',\n",
                            "   'DET',\n",
                            "   'ADJ',\n",
                            "   'NOUN',\n",
                            "   'ADP',\n",
                            "   'DET',\n",
                            "   'ADJ',\n",
                            "   'NOUN',\n",
                            "   'ADP',\n",
                            "   'DET',\n",
                            "   'PROPN',\n",
                            "   'PROPN',\n",
                            "   'PUNCT',\n",
                            "   'PRON',\n",
                            "   'PRON',\n",
                            "   'VERB',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'PUNCT'],\n",
                            "  'tagged_tokens': ['In_ADP',\n",
                            "   '1599_NUM',\n",
                            "   ',_PUNCT',\n",
                            "   'a_DET',\n",
                            "   'partnership_NOUN',\n",
                            "   'of_ADP',\n",
                            "   'company_NOUN',\n",
                            "   'members_NOUN',\n",
                            "   'built_VERB',\n",
                            "   'their_DET',\n",
                            "   'own_ADJ',\n",
                            "   'theatre_NOUN',\n",
                            "   'on_ADP',\n",
                            "   'the_DET',\n",
                            "   'south_ADJ',\n",
                            "   'bank_NOUN',\n",
                            "   'of_ADP',\n",
                            "   'the_DET',\n",
                            "   'River_PROPN',\n",
                            "   'Thames_PROPN',\n",
                            "   ',_PUNCT',\n",
                            "   'which_PRON',\n",
                            "   'they_PRON',\n",
                            "   'called_VERB',\n",
                            "   'the_DET',\n",
                            "   'Globe_NOUN',\n",
                            "   '._PUNCT']},\n",
                            " {'tokens': ['There',\n",
                            "   'he',\n",
                            "   'is',\n",
                            "   ',',\n",
                            "   'built',\n",
                            "   'like',\n",
                            "   'King',\n",
                            "   'Kong',\n",
                            "   ',',\n",
                            "   'as',\n",
                            "   'ambitious',\n",
                            "   'as',\n",
                            "   'the',\n",
                            "   'Empire',\n",
                            "   'State',\n",
                            "   'Building',\n",
                            "   ',',\n",
                            "   'as',\n",
                            "   'wide-eyed',\n",
                            "   'as',\n",
                            "   'Fay',\n",
                            "   'Wray',\n",
                            "   ',',\n",
                            "   'and',\n",
                            "   'as',\n",
                            "   'much',\n",
                            "   'a',\n",
                            "   'dream',\n",
                            "   ',',\n",
                            "   'an',\n",
                            "   'invention',\n",
                            "   ',',\n",
                            "   'as',\n",
                            "   'the',\n",
                            "   'movies',\n",
                            "   'and',\n",
                            "   'America',\n",
                            "   'itself',\n",
                            "   '.'],\n",
                            "  'tags': ['ADV',\n",
                            "   'PRON',\n",
                            "   'VERB',\n",
                            "   'PUNCT',\n",
                            "   'VERB',\n",
                            "   'ADP',\n",
                            "   'PROPN',\n",
                            "   'PROPN',\n",
                            "   'PUNCT',\n",
                            "   'ADV',\n",
                            "   'ADJ',\n",
                            "   'ADP',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'NOUN',\n",
                            "   'NOUN',\n",
                            "   'PUNCT',\n",
                            "   'ADV',\n",
                            "   'ADJ',\n",
                            "   'ADP',\n",
                            "   'PROPN',\n",
                            "   'PROPN',\n",
                            "   'PUNCT',\n",
                            "   'CCONJ',\n",
                            "   'ADV',\n",
                            "   'ADV',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'PUNCT',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'PUNCT',\n",
                            "   'SCONJ',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'CCONJ',\n",
                            "   'PROPN',\n",
                            "   'PRON',\n",
                            "   'PUNCT'],\n",
                            "  'tagged_tokens': ['There_ADV',\n",
                            "   'he_PRON',\n",
                            "   'is_VERB',\n",
                            "   ',_PUNCT',\n",
                            "   'built_VERB',\n",
                            "   'like_ADP',\n",
                            "   'King_PROPN',\n",
                            "   'Kong_PROPN',\n",
                            "   ',_PUNCT',\n",
                            "   'as_ADV',\n",
                            "   'ambitious_ADJ',\n",
                            "   'as_ADP',\n",
                            "   'the_DET',\n",
                            "   'Empire_NOUN',\n",
                            "   'State_NOUN',\n",
                            "   'Building_NOUN',\n",
                            "   ',_PUNCT',\n",
                            "   'as_ADV',\n",
                            "   'wide-eyed_ADJ',\n",
                            "   'as_ADP',\n",
                            "   'Fay_PROPN',\n",
                            "   'Wray_PROPN',\n",
                            "   ',_PUNCT',\n",
                            "   'and_CCONJ',\n",
                            "   'as_ADV',\n",
                            "   'much_ADV',\n",
                            "   'a_DET',\n",
                            "   'dream_NOUN',\n",
                            "   ',_PUNCT',\n",
                            "   'an_DET',\n",
                            "   'invention_NOUN',\n",
                            "   ',_PUNCT',\n",
                            "   'as_SCONJ',\n",
                            "   'the_DET',\n",
                            "   'movies_NOUN',\n",
                            "   'and_CCONJ',\n",
                            "   'America_PROPN',\n",
                            "   'itself_PRON',\n",
                            "   '._PUNCT']},\n",
                            " {'tokens': ['You',\n",
                            "   'see',\n",
                            "   ',',\n",
                            "   'I',\n",
                            "   'could',\n",
                            "   'have',\n",
                            "   'rested',\n",
                            "   'there',\n",
                            "   'beside',\n",
                            "   'her',\n",
                            "   ',',\n",
                            "   'perhaps',\n",
                            "   'forever',\n",
                            "   ',',\n",
                            "   'it',\n",
                            "   'felt',\n",
                            "   'like',\n",
                            "   'forever',\n",
                            "   ',',\n",
                            "   'a',\n",
                            "   'mirror',\n",
                            "   'confusion',\n",
                            "   'of',\n",
                            "   'bodies',\n",
                            "   'and',\n",
                            "   'sighs',\n",
                            "   ',',\n",
                            "   'undifferentiated',\n",
                            "   ',',\n",
                            "   'she',\n",
                            "   'in',\n",
                            "   'me',\n",
                            "   ',',\n",
                            "   'me',\n",
                            "   'in',\n",
                            "   'she',\n",
                            "   'and',\n",
                            "   'no',\n",
                            "   'longer',\n",
                            "   'exhausted',\n",
                            "   'by',\n",
                            "   'someone',\n",
                            "   'else',\n",
                            "   \"'s\",\n",
                            "   'shape',\n",
                            "   'over',\n",
                            "   'mine',\n",
                            "   '.'],\n",
                            "  'tags': ['PRON',\n",
                            "   'VERB',\n",
                            "   'PUNCT',\n",
                            "   'PRON',\n",
                            "   'AUX',\n",
                            "   'AUX',\n",
                            "   'VERB',\n",
                            "   'ADV',\n",
                            "   'ADP',\n",
                            "   'PRON',\n",
                            "   'PUNCT',\n",
                            "   'ADV',\n",
                            "   'ADV',\n",
                            "   'PUNCT',\n",
                            "   'PRON',\n",
                            "   'VERB',\n",
                            "   'ADP',\n",
                            "   'ADV',\n",
                            "   'PUNCT',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'NOUN',\n",
                            "   'ADP',\n",
                            "   'NOUN',\n",
                            "   'CCONJ',\n",
                            "   'NOUN',\n",
                            "   'PUNCT',\n",
                            "   'VERB',\n",
                            "   'PUNCT',\n",
                            "   'PRON',\n",
                            "   'ADP',\n",
                            "   'PRON',\n",
                            "   'PUNCT',\n",
                            "   'PRON',\n",
                            "   'ADP',\n",
                            "   'PRON',\n",
                            "   'CCONJ',\n",
                            "   'DET',\n",
                            "   'ADV',\n",
                            "   'ADJ',\n",
                            "   'ADP',\n",
                            "   'PRON',\n",
                            "   'PRON',\n",
                            "   'PART',\n",
                            "   'NOUN',\n",
                            "   'ADP',\n",
                            "   'PRON',\n",
                            "   'PUNCT'],\n",
                            "  'tagged_tokens': ['You_PRON',\n",
                            "   'see_VERB',\n",
                            "   ',_PUNCT',\n",
                            "   'I_PRON',\n",
                            "   'could_AUX',\n",
                            "   'have_AUX',\n",
                            "   'rested_VERB',\n",
                            "   'there_ADV',\n",
                            "   'beside_ADP',\n",
                            "   'her_PRON',\n",
                            "   ',_PUNCT',\n",
                            "   'perhaps_ADV',\n",
                            "   'forever_ADV',\n",
                            "   ',_PUNCT',\n",
                            "   'it_PRON',\n",
                            "   'felt_VERB',\n",
                            "   'like_ADP',\n",
                            "   'forever_ADV',\n",
                            "   ',_PUNCT',\n",
                            "   'a_DET',\n",
                            "   'mirror_NOUN',\n",
                            "   'confusion_NOUN',\n",
                            "   'of_ADP',\n",
                            "   'bodies_NOUN',\n",
                            "   'and_CCONJ',\n",
                            "   'sighs_NOUN',\n",
                            "   ',_PUNCT',\n",
                            "   'undifferentiated_VERB',\n",
                            "   ',_PUNCT',\n",
                            "   'she_PRON',\n",
                            "   'in_ADP',\n",
                            "   'me_PRON',\n",
                            "   ',_PUNCT',\n",
                            "   'me_PRON',\n",
                            "   'in_ADP',\n",
                            "   'she_PRON',\n",
                            "   'and_CCONJ',\n",
                            "   'no_DET',\n",
                            "   'longer_ADV',\n",
                            "   'exhausted_ADJ',\n",
                            "   'by_ADP',\n",
                            "   'someone_PRON',\n",
                            "   'else_PRON',\n",
                            "   \"'s_PART\",\n",
                            "   'shape_NOUN',\n",
                            "   'over_ADP',\n",
                            "   'mine_PRON',\n",
                            "   '._PUNCT']},\n",
                            " {'tokens': ['The',\n",
                            "   'Persians',\n",
                            "   'call',\n",
                            "   'it',\n",
                            "   '\"',\n",
                            "   'Nesf-e-Jahan',\n",
                            "   '\"',\n",
                            "   ',',\n",
                            "   'meaning',\n",
                            "   '\"',\n",
                            "   'Half',\n",
                            "   'The',\n",
                            "   'World',\n",
                            "   '\"',\n",
                            "   '.'],\n",
                            "  'tags': ['DET',\n",
                            "   'NOUN',\n",
                            "   'VERB',\n",
                            "   'PRON',\n",
                            "   'PUNCT',\n",
                            "   'X',\n",
                            "   'PUNCT',\n",
                            "   'PUNCT',\n",
                            "   'VERB',\n",
                            "   'PUNCT',\n",
                            "   'PRON',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'PUNCT',\n",
                            "   'PUNCT'],\n",
                            "  'tagged_tokens': ['The_DET',\n",
                            "   'Persians_NOUN',\n",
                            "   'call_VERB',\n",
                            "   'it_PRON',\n",
                            "   '\"_PUNCT',\n",
                            "   'Nesf-e-Jahan_X',\n",
                            "   '\"_PUNCT',\n",
                            "   ',_PUNCT',\n",
                            "   'meaning_VERB',\n",
                            "   '\"_PUNCT',\n",
                            "   'Half_PRON',\n",
                            "   'The_DET',\n",
                            "   'World_NOUN',\n",
                            "   '\"_PUNCT',\n",
                            "   '._PUNCT']},\n",
                            " {'tokens': ['This',\n",
                            "   'is',\n",
                            "   'unlike',\n",
                            "   'the',\n",
                            "   'situation',\n",
                            "   'last',\n",
                            "   'year',\n",
                            "   'in',\n",
                            "   'Asia',\n",
                            "   'when',\n",
                            "   'we',\n",
                            "   'evacuated',\n",
                            "   'U.S.',\n",
                            "   'citizens',\n",
                            "   'from',\n",
                            "   'areas',\n",
                            "   'that',\n",
                            "   'were',\n",
                            "   'hit',\n",
                            "   'by',\n",
                            "   'the',\n",
                            "   'tsunami',\n",
                            "   '-',\n",
                            "   'a',\n",
                            "   'phenomenon',\n",
                            "   'that',\n",
                            "   'is',\n",
                            "   'much',\n",
                            "   'less',\n",
                            "   'predictable',\n",
                            "   'than',\n",
                            "   'the',\n",
                            "   'Hezbollah',\n",
                            "   '-',\n",
                            "   'provoked',\n",
                            "   'destruction',\n",
                            "   'that',\n",
                            "   'rained',\n",
                            "   'down',\n",
                            "   'on',\n",
                            "   'Lebanon',\n",
                            "   '.'],\n",
                            "  'tags': ['PRON',\n",
                            "   'AUX',\n",
                            "   'ADP',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'ADJ',\n",
                            "   'NOUN',\n",
                            "   'ADP',\n",
                            "   'PROPN',\n",
                            "   'ADV',\n",
                            "   'PRON',\n",
                            "   'VERB',\n",
                            "   'PROPN',\n",
                            "   'NOUN',\n",
                            "   'ADP',\n",
                            "   'NOUN',\n",
                            "   'PRON',\n",
                            "   'AUX',\n",
                            "   'VERB',\n",
                            "   'ADP',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'PUNCT',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'PRON',\n",
                            "   'AUX',\n",
                            "   'ADV',\n",
                            "   'ADV',\n",
                            "   'ADJ',\n",
                            "   'ADP',\n",
                            "   'DET',\n",
                            "   'PROPN',\n",
                            "   'PUNCT',\n",
                            "   'VERB',\n",
                            "   'NOUN',\n",
                            "   'PRON',\n",
                            "   'VERB',\n",
                            "   'ADV',\n",
                            "   'ADP',\n",
                            "   'PROPN',\n",
                            "   'PUNCT'],\n",
                            "  'tagged_tokens': ['This_PRON',\n",
                            "   'is_AUX',\n",
                            "   'unlike_ADP',\n",
                            "   'the_DET',\n",
                            "   'situation_NOUN',\n",
                            "   'last_ADJ',\n",
                            "   'year_NOUN',\n",
                            "   'in_ADP',\n",
                            "   'Asia_PROPN',\n",
                            "   'when_ADV',\n",
                            "   'we_PRON',\n",
                            "   'evacuated_VERB',\n",
                            "   'U.S._PROPN',\n",
                            "   'citizens_NOUN',\n",
                            "   'from_ADP',\n",
                            "   'areas_NOUN',\n",
                            "   'that_PRON',\n",
                            "   'were_AUX',\n",
                            "   'hit_VERB',\n",
                            "   'by_ADP',\n",
                            "   'the_DET',\n",
                            "   'tsunami_NOUN',\n",
                            "   '-_PUNCT',\n",
                            "   'a_DET',\n",
                            "   'phenomenon_NOUN',\n",
                            "   'that_PRON',\n",
                            "   'is_AUX',\n",
                            "   'much_ADV',\n",
                            "   'less_ADV',\n",
                            "   'predictable_ADJ',\n",
                            "   'than_ADP',\n",
                            "   'the_DET',\n",
                            "   'Hezbollah_PROPN',\n",
                            "   '-_PUNCT',\n",
                            "   'provoked_VERB',\n",
                            "   'destruction_NOUN',\n",
                            "   'that_PRON',\n",
                            "   'rained_VERB',\n",
                            "   'down_ADV',\n",
                            "   'on_ADP',\n",
                            "   'Lebanon_PROPN',\n",
                            "   '._PUNCT']},\n",
                            " {'tokens': ['that',\n",
                            "   'the',\n",
                            "   'citizen',\n",
                            "   'has',\n",
                            "   'not',\n",
                            "   'that',\n",
                            "   'protection',\n",
                            "   'of',\n",
                            "   'person',\n",
                            "   'and',\n",
                            "   'property',\n",
                            "   'which',\n",
                            "   'he',\n",
                            "   'is',\n",
                            "   'entitled',\n",
                            "   '.'],\n",
                            "  'tags': ['SCONJ',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'VERB',\n",
                            "   'PART',\n",
                            "   'DET',\n",
                            "   'NOUN',\n",
                            "   'ADP',\n",
                            "   'NOUN',\n",
                            "   'CCONJ',\n",
                            "   'NOUN',\n",
                            "   'PRON',\n",
                            "   'PRON',\n",
                            "   'AUX',\n",
                            "   'VERB',\n",
                            "   'PUNCT'],\n",
                            "  'tagged_tokens': ['that_SCONJ',\n",
                            "   'the_DET',\n",
                            "   'citizen_NOUN',\n",
                            "   'has_VERB',\n",
                            "   'not_PART',\n",
                            "   'that_DET',\n",
                            "   'protection_NOUN',\n",
                            "   'of_ADP',\n",
                            "   'person_NOUN',\n",
                            "   'and_CCONJ',\n",
                            "   'property_NOUN',\n",
                            "   'which_PRON',\n",
                            "   'he_PRON',\n",
                            "   'is_AUX',\n",
                            "   'entitled_VERB',\n",
                            "   '._PUNCT']},\n",
                            " {'tokens': ['Great', 'place', '5', 'stars', 'for', 'sure', '.'],\n",
                            "  'tags': ['ADJ', 'NOUN', 'NUM', 'NOUN', 'ADP', 'ADJ', 'PUNCT'],\n",
                            "  'tagged_tokens': ['Great_ADJ',\n",
                            "   'place_NOUN',\n",
                            "   '5_NUM',\n",
                            "   'stars_NOUN',\n",
                            "   'for_ADP',\n",
                            "   'sure_ADJ',\n",
                            "   '._PUNCT']},\n",
                            " {'tokens': ['Now',\n",
                            "   'that',\n",
                            "   'the',\n",
                            "   'Dursleys',\n",
                            "   'knew',\n",
                            "   'they',\n",
                            "   'were',\n",
                            "   \"n't\",\n",
                            "   'going',\n",
                            "   'to',\n",
                            "   'wake',\n",
                            "   'up',\n",
                            "   'as',\n",
                            "   'fruitbats',\n",
                            "   ',',\n",
                            "   'he',\n",
                            "   'had',\n",
                            "   'lost',\n",
                            "   'his',\n",
                            "   'only',\n",
                            "   'weapon',\n",
                            "   '.'],\n",
                            "  'tags': ['ADV',\n",
                            "   'SCONJ',\n",
                            "   'DET',\n",
                            "   'PROPN',\n",
                            "   'VERB',\n",
                            "   'PRON',\n",
                            "   'AUX',\n",
                            "   'PART',\n",
                            "   'VERB',\n",
                            "   'PART',\n",
                            "   'VERB',\n",
                            "   'ADV',\n",
                            "   'ADP',\n",
                            "   'NOUN',\n",
                            "   'PUNCT',\n",
                            "   'PRON',\n",
                            "   'AUX',\n",
                            "   'VERB',\n",
                            "   'PRON',\n",
                            "   'ADJ',\n",
                            "   'NOUN',\n",
                            "   'PUNCT'],\n",
                            "  'tagged_tokens': ['Now_ADV',\n",
                            "   'that_SCONJ',\n",
                            "   'the_DET',\n",
                            "   'Dursleys_PROPN',\n",
                            "   'knew_VERB',\n",
                            "   'they_PRON',\n",
                            "   'were_AUX',\n",
                            "   \"n't_PART\",\n",
                            "   'going_VERB',\n",
                            "   'to_PART',\n",
                            "   'wake_VERB',\n",
                            "   'up_ADV',\n",
                            "   'as_ADP',\n",
                            "   'fruitbats_NOUN',\n",
                            "   ',_PUNCT',\n",
                            "   'he_PRON',\n",
                            "   'had_AUX',\n",
                            "   'lost_VERB',\n",
                            "   'his_PRON',\n",
                            "   'only_ADJ',\n",
                            "   'weapon_NOUN',\n",
                            "   '._PUNCT']}]"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "train_examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "eb6ae597",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['AUX',\n",
                            " 'PROPN',\n",
                            " 'DET',\n",
                            " 'VERB',\n",
                            " 'ADV',\n",
                            " 'ADP',\n",
                            " 'PUNCT',\n",
                            " 'X',\n",
                            " 'ADJ',\n",
                            " 'PRON',\n",
                            " 'SCONJ',\n",
                            " 'NUM',\n",
                            " 'CCONJ',\n",
                            " 'NOUN',\n",
                            " 'PART']"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "valid_labels = set()\n",
                "for example in train_examples:\n",
                "    valid_labels.update(example[\"tags\"])\n",
                "valid_labels = list(valid_labels)\n",
                "valid_labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "7fd344b8",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[{'role': 'system',\n",
                            "  'content': 'Do not try to answer the question. Just tag each token in the sentence.'},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"In 1599 , a partnership of company members built their own theatre on the south bank of the River Thames , which they called the Globe .\"'},\n",
                            " {'role': 'assistant',\n",
                            "  'content': 'In_ADP 1599_NUM ,_PUNCT a_DET partnership_NOUN of_ADP company_NOUN members_NOUN built_VERB their_DET own_ADJ theatre_NOUN on_ADP the_DET south_ADJ bank_NOUN of_ADP the_DET River_PROPN Thames_PROPN ,_PUNCT which_PRON they_PRON called_VERB the_DET Globe_NOUN ._PUNCT'},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"There he is , built like King Kong , as ambitious as the Empire State Building , as wide-eyed as Fay Wray , and as much a dream , an invention , as the movies and America itself .\"'},\n",
                            " {'role': 'assistant',\n",
                            "  'content': 'There_ADV he_PRON is_VERB ,_PUNCT built_VERB like_ADP King_PROPN Kong_PROPN ,_PUNCT as_ADV ambitious_ADJ as_ADP the_DET Empire_NOUN State_NOUN Building_NOUN ,_PUNCT as_ADV wide-eyed_ADJ as_ADP Fay_PROPN Wray_PROPN ,_PUNCT and_CCONJ as_ADV much_ADV a_DET dream_NOUN ,_PUNCT an_DET invention_NOUN ,_PUNCT as_SCONJ the_DET movies_NOUN and_CCONJ America_PROPN itself_PRON ._PUNCT'},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"You see , I could have rested there beside her , perhaps forever , it felt like forever , a mirror confusion of bodies and sighs , undifferentiated , she in me , me in she and no longer exhausted by someone else \\'s shape over mine .\"'},\n",
                            " {'role': 'assistant',\n",
                            "  'content': \"You_PRON see_VERB ,_PUNCT I_PRON could_AUX have_AUX rested_VERB there_ADV beside_ADP her_PRON ,_PUNCT perhaps_ADV forever_ADV ,_PUNCT it_PRON felt_VERB like_ADP forever_ADV ,_PUNCT a_DET mirror_NOUN confusion_NOUN of_ADP bodies_NOUN and_CCONJ sighs_NOUN ,_PUNCT undifferentiated_VERB ,_PUNCT she_PRON in_ADP me_PRON ,_PUNCT me_PRON in_ADP she_PRON and_CCONJ no_DET longer_ADV exhausted_ADJ by_ADP someone_PRON else_PRON 's_PART shape_NOUN over_ADP mine_PRON ._PUNCT\"},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"The Persians call it \" Nesf-e-Jahan \" , meaning \" Half The World \" .\"'},\n",
                            " {'role': 'assistant',\n",
                            "  'content': 'The_DET Persians_NOUN call_VERB it_PRON \"_PUNCT Nesf-e-Jahan_X \"_PUNCT ,_PUNCT meaning_VERB \"_PUNCT Half_PRON The_DET World_NOUN \"_PUNCT ._PUNCT'},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"This is unlike the situation last year in Asia when we evacuated U.S. citizens from areas that were hit by the tsunami - a phenomenon that is much less predictable than the Hezbollah - provoked destruction that rained down on Lebanon .\"'},\n",
                            " {'role': 'assistant',\n",
                            "  'content': 'This_PRON is_AUX unlike_ADP the_DET situation_NOUN last_ADJ year_NOUN in_ADP Asia_PROPN when_ADV we_PRON evacuated_VERB U.S._PROPN citizens_NOUN from_ADP areas_NOUN that_PRON were_AUX hit_VERB by_ADP the_DET tsunami_NOUN -_PUNCT a_DET phenomenon_NOUN that_PRON is_AUX much_ADV less_ADV predictable_ADJ than_ADP the_DET Hezbollah_PROPN -_PUNCT provoked_VERB destruction_NOUN that_PRON rained_VERB down_ADV on_ADP Lebanon_PROPN ._PUNCT'},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"that the citizen has not that protection of person and property which he is entitled .\"'},\n",
                            " {'role': 'assistant',\n",
                            "  'content': 'that_SCONJ the_DET citizen_NOUN has_VERB not_PART that_DET protection_NOUN of_ADP person_NOUN and_CCONJ property_NOUN which_PRON he_PRON is_AUX entitled_VERB ._PUNCT'},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"Great place 5 stars for sure .\"'},\n",
                            " {'role': 'assistant',\n",
                            "  'content': 'Great_ADJ place_NOUN 5_NUM stars_NOUN for_ADP sure_ADJ ._PUNCT'},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"Now that the Dursleys knew they were n\\'t going to wake up as fruitbats , he had lost his only weapon .\"'},\n",
                            " {'role': 'assistant',\n",
                            "  'content': \"Now_ADV that_SCONJ the_DET Dursleys_PROPN knew_VERB they_PRON were_AUX n't_PART going_VERB to_PART wake_VERB up_ADV as_ADP fruitbats_NOUN ,_PUNCT he_PRON had_AUX lost_VERB his_PRON only_ADJ weapon_NOUN ._PUNCT\"},\n",
                            " {'role': 'user',\n",
                            "  'content': 'Tag the following sentence: \"Now , none of this has been confirmed by Google at the present time , but it \\'s an old adage that you follow the money to see who is behind something .\"'}]"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "test_example = test_dataset[186]\n",
                "\n",
                "prompt, label = construct_tagging_prompt(\n",
                "    train_examples,\n",
                "    test_example,\n",
                "    prompt_template=prompt_template,\n",
                "    chat_prompt=True,\n",
                "    instruction=\"Do not try to answer the question. Just tag each token in the sentence.\"\n",
                ")\n",
                "prompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2aaf8f89",
            "metadata": {},
            "outputs": [],
            "source": [
                "test_example[\"tokens\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cea167e5",
            "metadata": {},
            "outputs": [],
            "source": [
                "preds = gpt3x_completion(\n",
                "    prompt,\n",
                "    model,\n",
                "    test_example[\"tokens\"],\n",
                "    temperature=0,\n",
                "    max_tokens=100\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f4cd2f83",
            "metadata": {},
            "outputs": [],
            "source": [
                "preds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e9c97119",
            "metadata": {},
            "outputs": [],
            "source": [
                "\" \".join(test_example[\"tagged_tokens\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "afe26b32",
            "metadata": {},
            "outputs": [],
            "source": [
                "test_example = test_dataset[0]\n",
                "prompt, label = construct_tagging_prompt(\n",
                "    train_examples,\n",
                "    test_example,\n",
                "    prompt_template=prompt_template,\n",
                "    chat_prompt=False,\n",
                "    instruction=instruction\n",
                ")\n",
                "print(prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02ecd577",
            "metadata": {},
            "outputs": [],
            "source": [
                "preds = gpt3x_tagger(\n",
                "    prompt,\n",
                "    model,\n",
                "    test_example[\"tokens\"],\n",
                "    one_shot_tag=False,\n",
                "    temperature=0,\n",
                "    max_tokens=5\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28a880b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "preds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c3cc37a",
            "metadata": {},
            "outputs": [],
            "source": [
                "test_example[\"tags\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0f4a7eba",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_model_pred(\n",
                "    train_examples,\n",
                "    test_example,\n",
                "    prompt_template,\n",
                "    verbalizer={},\n",
                "    model=model,\n",
                "    chat_prompt=True,\n",
                "    instruction=instruction,\n",
                "    one_shot_tag=True,\n",
                "    max_tokens=max_tokens\n",
                "    \n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "08247234",
            "metadata": {},
            "outputs": [],
            "source": [
                "preds = [pred if pred != \"\" else np.random.choice(valid_labels) for pred in preds]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed55b5c2",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(preds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13e0413c",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Prediction: {preds}\")\n",
                "print(f\"Label: {label}\")\n",
                "\n",
                "f1_score([preds], [label])\n",
                "\n",
                "# prediction = {\"prediction_text\": pred, \"id\": test_example[\"id\"]}\n",
                "# reference = {}\n",
                "# reference[\"answers\"] = test_example[\"answers\"]\n",
                "# reference[\"id\"] = test_example[\"id\"]\n",
                "# results = squad_metric.compute(\n",
                "#             predictions=[prediction],\n",
                "#             references=[reference]\n",
                "#         )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bc555f12",
            "metadata": {},
            "outputs": [],
            "source": [
                "f1_sum = 0\n",
                "em_sum = 0\n",
                "avg_em = 0\n",
                "avg_f1 = 0\n",
                "\n",
                "run_details = {\"num_calls\": 0}\n",
                "\n",
                "pbar = tqdm(enumerate(test_dataset.select(range(1000))))\n",
                "\n",
                "for i, test_example in pbar:    \n",
                "    prompt, label = construct_tagging_prompt(\n",
                "        train_examples,\n",
                "        test_example,\n",
                "        prompt_template=prompt_template,\n",
                "        chat_prompt=True,\n",
                "        instruction=instruction\n",
                "    )\n",
                "    preds = gpt3x_tagger(\n",
                "        prompt,\n",
                "        model,\n",
                "        test_example[\"tokens\"],\n",
                "        one_shot_tag=True,\n",
                "        temperature=0,\n",
                "        max_tokens=100\n",
                "    )\n",
                "    preds = [pred if pred != \"\" else np.random.choice(valid_labels) for pred in preds]\n",
                "    f1_sum += f1_score([preds], [label])\n",
                "        \n",
                "    avg_f1 = f1_sum / (i+1)\n",
                "    \n",
                "#     wandb.log({\"f1\": avg_f1})\n",
                "#     wandb.log(run_details)\n",
                "    pbar.set_description(f\"f1: {avg_f1}\")\n",
                "#     time.sleep(1/2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b220e468",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}