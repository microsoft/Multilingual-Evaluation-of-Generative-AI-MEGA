{
    "prompt": [
        {
            "role": "user",
            "content": "Please fill the dataset card for the WikiANN dataset. You can use the following schema:\nlanguages: \n- \"List of ISO 639-1 code for languages covered in the dataset\"\n- lang1\n- lang2\n...\n- langM\npretty_name: \"Pretty Name of the Dataset\"\ntags:\n- tag1\n- tag2\ndataset_info:\n    features:\n        - name: \"name of the feature1\"\n          dtype: \"dtype of the feature1\"\n        ...\n        - name: \"name of the featureN\"\n          dtype: \"dtype of the featureN\"\n    splits:\n        train:\n            num_examples: \"number of examples in the train split\"\n        validation:\n            num_examples: \"number of examples in the validation split\"\n        test:\n            num_examples: \"number of examples in the test split\"\ndataset_summary: \"Summary of the dataset's description\"\n"
        }
    ],
    "response": "languages: \n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- ga\n- gl\n- gu\n- he\n- hi\n- hr\n- ht\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lb\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- mt\n- my\n- ne\n- nl\n- no\n- oc\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- sh\n- si\n- sk\n- sl\n- sq\n- sr\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zu\n\npretty_name: \"WikiANN\"\n\ntags:\n- named_entity_recognition\n- multilingual\n\ndataset_info:\n    features:\n        - name: \"tokens\"\n          dtype: \"List of strings\"\n        - name: \"ner_tags\"\n          dtype: \"List of integers\"\n        - name: \"langs\"\n          dtype: \"List of strings\"\n\n    splits:\n        train:\n            num_examples: \"1,000,000\"\n        validation:\n            num_examples: \"200,000\"\n        test:\n            num_examples: \"200,000\"\n\ndataset_summary: \"WikiANN is a multilingual named entity recognition dataset consisting of over 1.4 million annotated sentences in 282 languages. The dataset is derived from Wikipedia articles and is designed for training and evaluating NER models across multiple languages.\""
}