{
    "prompt": [
        {
            "role": "user",
            "content": "Please fill the dataset card for the MLQA dataset. You can use the following schema:\nlanguages: \n- \"List of ISO 639-1 code for languages covered in the dataset\"\n- lang1\n- lang2\n...\n- langM\npretty_name: \"Pretty Name of the Dataset\"\ntags:\n- tag1\n- tag2\ndataset_info:\n    features:\n        - name: \"name of the feature1\"\n          dtype: \"dtype of the feature1\"\n        ...\n        - name: \"name of the featureN\"\n          dtype: \"dtype of the featureN\"\n    splits:\n        train:\n            num_examples: \"number of examples in the train split\"\n        validation:\n            num_examples: \"number of examples in the validation split\"\n        test:\n            num_examples: \"number of examples in the test split\"\ndataset_summary: \"Summary of the dataset's description\"\n and the following example:\nlanguage:\n  - ar\n  - bg\n  - de\n  - el\n  - en\n  - es\n  - fr\n  - hi\n  - ru\n  - sw\n  - th\n  - tr\n  - ur\n  - vi\n  - zh\npaperswithcode_id: xnli\npretty_name: Cross-lingual Natural Language Inference\ndataset_info:\n  features:\n    - name: premise\n      dtype: string\n    - name: hypothesis\n      dtype: string\n    - name: label\n      dtype:\n        class_label:\n          names:\n            \"0\": entailment\n            \"1\": neutral\n            \"2\": contradiction\n  splits:\n    train:\n      num_examples: 392702\n    validation:\n      num_examples: 2490\n    test:\n      num_examples: 5010\ndataset_summary: XNLI is a subset of a few thousand examples from MNLI which has been translated\ninto a 14 different languages (some low-ish resource). As with MNLI, the goal is\nto predict textual entailment (does sentence A imply/contradict/neither sentence\nB) and is a classification task (given two sentences, predict one of three\nlabels).\n"
        }
    ],
    "response": "languages:\n  - ar\n  - de\n  - en\n  - es\n  - hi\n  - vi\n  - zh\npretty_name: MLQA\ntags:\n  - question_answering\n  - multilingual\ndataset_info:\n  features:\n    - name: context\n      dtype: string\n    - name: question\n      dtype: string\n    - name: answers\n      dtype:\n        list:\n          item:\n            dict:\n              - key: text\n                value: string\n              - key: answer_start\n                value: int\n  splits:\n    train:\n      num_examples: 0\n    validation:\n      num_examples: 7199\n    test:\n      num_examples: 0\ndataset_summary: MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance. The dataset contains over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between 4 different languages on average."
}