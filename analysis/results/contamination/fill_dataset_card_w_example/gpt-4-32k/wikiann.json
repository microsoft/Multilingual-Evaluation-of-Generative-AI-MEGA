{
    "prompt": [
        {
            "role": "user",
            "content": "Please fill the dataset card for the WikiANN dataset. You can use the following schema:\nlanguages: \n- \"List of ISO 639-1 code for languages covered in the dataset\"\n- lang1\n- lang2\n...\n- langM\npretty_name: \"Pretty Name of the Dataset\"\ntags:\n- tag1\n- tag2\ndataset_info:\n    features:\n        - name: \"name of the feature1\"\n          dtype: \"dtype of the feature1\"\n        ...\n        - name: \"name of the featureN\"\n          dtype: \"dtype of the featureN\"\n    splits:\n        train:\n            num_examples: \"number of examples in the train split\"\n        validation:\n            num_examples: \"number of examples in the validation split\"\n        test:\n            num_examples: \"number of examples in the test split\"\ndataset_summary: \"Summary of the dataset's description\"\n and the following example:\nlanguage:\n  - ar\n  - bg\n  - de\n  - el\n  - en\n  - es\n  - fr\n  - hi\n  - ru\n  - sw\n  - th\n  - tr\n  - ur\n  - vi\n  - zh\npaperswithcode_id: xnli\npretty_name: Cross-lingual Natural Language Inference\ndataset_info:\n  features:\n    - name: premise\n      dtype: string\n    - name: hypothesis\n      dtype: string\n    - name: label\n      dtype:\n        class_label:\n          names:\n            \"0\": entailment\n            \"1\": neutral\n            \"2\": contradiction\n  splits:\n    train:\n      num_examples: 392702\n    validation:\n      num_examples: 2490\n    test:\n      num_examples: 5010\ndataset_summary: XNLI is a subset of a few thousand examples from MNLI which has been translated\ninto a 14 different languages (some low-ish resource). As with MNLI, the goal is\nto predict textual entailment (does sentence A imply/contradict/neither sentence\nB) and is a classification task (given two sentences, predict one of three\nlabels).\n"
        }
    ],
    "response": "languages:\n  - af\n  - ar\n  - bg\n  - bn\n  - de\n  - el\n  - en\n  - es\n  - et\n  - eu\n  - fa\n  - fi\n  - fr\n  - he\n  - hi\n  - hr\n  - hu\n  - id\n  - it\n  - ja\n  - jv\n  - ka\n  - kk\n  - ko\n  - lt\n  - lv\n  - mk\n  - ml\n  - mr\n  - ms\n  - my\n  - nb\n  - nl\n  - pl\n  - pt\n  - ro\n  - ru\n  - si\n  - sk\n  - sl\n  - sq\n  - sr\n  - sv\n  - sw\n  - ta\n  - te\n  - th\n  - tl\n  - tr\n  - uk\n  - ur\n  - uz\n  - vi\n  - xh\n  - zh\n  - zu\npretty_name: WikiANN\ntags:\n  - named_entity_recognition\n  - multilingual\ndataset_info:\n  features:\n    - name: tokens\n      dtype: List[string]\n    - name: ner_tags\n      dtype: List[int]\n  splits:\n    train:\n      num_examples: 115424\n    validation:\n      num_examples: 5000\n    test:\n      num_examples: 5000\ndataset_summary: WikiANN is a multilingual named entity recognition dataset consisting of over 100,000 annotated sentences for 40 languages. The dataset is derived from Wikipedia articles, and the annotations are automatically generated by matching Wikipedia links with corresponding Wikidata items. The goal is to predict the named entity tags (e.g., person, location, organization) for each token in a given sentence."
}