{
    "prompt": [
        {
            "role": "user",
            "content": "Please fill the dataset card for the MLQA dataset. You can use the following schema:\nlanguages: \n- \"List of ISO 639-1 code for languages covered in the dataset\"\n- lang1\n- lang2\n...\n- langM\npretty_name: \"Pretty Name of the Dataset\"\ntags:\n- tag1\n- tag2\ndataset_info:\n    features:\n        - name: \"name of the feature1\"\n          dtype: \"dtype of the feature1\"\n        ...\n        - name: \"name of the featureN\"\n          dtype: \"dtype of the featureN\"\n    splits:\n        train:\n            num_examples: \"number of examples in the train split\"\n        validation:\n            num_examples: \"number of examples in the validation split\"\n        test:\n            num_examples: \"number of examples in the test split\"\ndataset_summary: \"Summary of the dataset's description\"\n and the following example:\nlanguage:\n  - ar\n  - bg\n  - de\n  - el\n  - en\n  - es\n  - fr\n  - hi\n  - ru\n  - sw\n  - th\n  - tr\n  - ur\n  - vi\n  - zh\npaperswithcode_id: xnli\npretty_name: Cross-lingual Natural Language Inference\ndataset_info:\n  features:\n    - name: premise\n      dtype: string\n    - name: hypothesis\n      dtype: string\n    - name: label\n      dtype:\n        class_label:\n          names:\n            \"0\": entailment\n            \"1\": neutral\n            \"2\": contradiction\n  splits:\n    train:\n      num_examples: 392702\n    validation:\n      num_examples: 2490\n    test:\n      num_examples: 5010\ndataset_summary: XNLI is a subset of a few thousand examples from MNLI which has been translated\ninto a 14 different languages (some low-ish resource). As with MNLI, the goal is\nto predict textual entailment (does sentence A imply/contradict/neither sentence\nB) and is a classification task (given two sentences, predict one of three\nlabels).\n"
        }
    ],
    "response": "languages:\n- en\n- ar\n- de\n- es\n- hi\n- vi\n- zh\npretty_name: Multilingual Question Answering\ntags:\n- question answering\n- multilingual\ndataset_info:\n    features:\n        - name: \"context\"\n          dtype: \"string\"\n        - name: \"question\"\n          dtype: \"string\"\n        - name: \"answers\"\n          dtype: \"List of dictionaries\"\n          value: \n            - \"text\": \"string\"\n              \"answer_start\": \"int\"\n        - name: \"id\"\n          dtype: \"string\"\n        - name: \"title\"\n          dtype: \"string\"\n    splits:\n        train:\n            num_examples: 86821\n        validation:\n            num_examples: 1140\n        test:\n            num_examples: 1140\ndataset_summary: MLQA is a benchmark dataset for cross-lingual question answering. It consists of questions and answers pairs in 7 languages (English, Arabic, German, Spanish, Hindi, Vietnamese, and Chinese) and their corresponding contexts. The dataset is designed to evaluate the ability of models to transfer knowledge across languages and answer questions in a multilingual setting."
}